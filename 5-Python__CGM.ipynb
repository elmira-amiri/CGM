{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "from ggplot import *\n",
    "import chart_studio.plotly as py\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib import rc\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_regression,  mutual_info_classif, chi2 \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    roc_curve,\n",
    "    matthews_corrcoef,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "import scipy\n",
    "\n",
    "# include if using a Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLIER_DETECTION = 'YES'     # ['YES', 'NO']\n",
    "NUM_RUNS = 10\n",
    "MAIN_DIR = '.'\n",
    "\n",
    "df_final_prediction = None\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_pickle(f'{MAIN_DIR}/main_data/tumour_gene_expression_and_clinical.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_rank(df, label):\n",
    "    T1= df[df[label]==0]['t.rfs']\n",
    "    E1=df[df[label]==0]['e.rfs']\n",
    "\n",
    "    T2= df[df[label]==1]['t.rfs']\n",
    "    E2=df[df[label]==1]['e.rfs']\n",
    "    return (T1,E1,T2,E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analyzed_data_from_R():\n",
    "    df_total = None\n",
    "\n",
    "    print('Loading cancer data...')\n",
    "    \n",
    "    # load all cancer data\n",
    "    df_total = pd.read_pickle(f'{MAIN_DIR}/main_data/tumour_gene_expression_and_clinical.pkl')\n",
    "\n",
    "    # load all normal data\n",
    "    df_total_normal = pd.read_pickle(f'{MAIN_DIR}/main_data/normal_gene_expression_and_clinical.pkl')\n",
    "\n",
    "    num_normal = df_total_normal.shape[0]\n",
    "    num_tumor = df_total.shape[0]\n",
    "    print(f\"\\x1b[1;30m Normal: {num_normal}     Tumor: {num_tumor} \\x1b[0m\")\n",
    "\n",
    "    # reset index\n",
    "    df_total.set_index(\"sample_id\")\n",
    "    df_total_normal.set_index(\"sample_id\")\n",
    "\n",
    "    return df_total_normal, df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_total, num_of_genes_to_select=None):\n",
    "    # deleting AFF\n",
    "    unwanted_Aff = df_total.filter(regex=\"^AFFX\")\n",
    "    print(f'Removing {unwanted_Aff.shape} rows with AFFX from df_total')\n",
    "    df_total.drop(unwanted_Aff, axis=1, inplace=True)\n",
    "\n",
    "    # deleting -at\n",
    "    unwanted_at = df_total.filter(regex=\"_at\")\n",
    "    print(f'Removing {unwanted_at.shape} rows with _at from df_total')\n",
    "    df_total.drop(unwanted_at, axis=1, inplace=True)\n",
    "\n",
    "    df_clean = df_total.drop(columns=[\"sample_id\"])\n",
    "        \n",
    "    # remove normal cells\n",
    "    df_clean = df_clean[df_clean.tissue != 0]\n",
    "  \n",
    "    # CGM genes\n",
    "    with open (f'{MAIN_DIR}/main_data/CGM_biomarkers.pkl', 'rb') as fp:\n",
    "        gene_list = pickle.load(fp)\n",
    "        gene_list_selected_number = gene_list[0:num_of_genes_to_select]\n",
    "        \n",
    "    annotations = ['tissue', 'stage', 'grade','subtype','PAM50','SCMOD2', 'tumor size(mm)', 'age at diagnosis(yrs)',\n",
    "                   'death event time(yrs)', 'death event', 'er', 'pr', 'her2', 't.rfs', 'e.rfs', 't.tdm','e.tdm',\n",
    "                   'EndoPredict','oncotypedx','GGI']\n",
    "    \n",
    "    df_genes = gene_list_selected_number + annotations\n",
    "    df_clean = df_clean[df_genes]\n",
    "       \n",
    "    print('Setting aside grade 2 and null-grades for final prediction')\n",
    "    global df_final_prediction\n",
    "    df_final_prediction = df_clean[(df_clean[\"grade\"] == 2) | (df_clean[\"grade\"].isnull())].copy()\n",
    "    \n",
    "    print('Selecting just columns that have grade')\n",
    "    df_clean = df_clean.loc[df_clean.grade.notnull()]\n",
    "    print(f'shape of df_final_prediction: {df_final_prediction.shape}')\n",
    "  \n",
    "    df_clean = df_clean[df_clean.grade != 2]\n",
    "    df_clean = df_clean.replace({\"grade\": {1: 0}})\n",
    "    df_clean = df_clean.replace({\"grade\": {3: 1}})    \n",
    "    \n",
    "    print (f\"Num of each grade: {df_clean['grade'].value_counts()}\") \n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(num_of_genes_to_select=None):\n",
    "    df_total_normal, df_total = load_analyzed_data_from_R()\n",
    "    \n",
    "    return clean_data(df_total_normal, num_of_genes_to_select), clean_data(df_total, num_of_genes_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_normal, df_total = load_analyzed_data_from_R()\n",
    "df_clean_normal, df = read_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "df_total = df_total.replace({'er':{' NA':np.nan,' EV':np.nan, '1':1, '0':0},\n",
    "                               'pr':{' NA':np.nan,' EV':np.nan, '1':1, '0':0},\n",
    "                               'her2':{' NA':np.nan,' EV':np.nan, '1':1, '0':0},\n",
    "                               'stage':{0:np.nan, ' NA':np.nan, 0:np.nan},\n",
    "                               'tumor size(mm)':{'>= 20':25, '< 20':10, ' --':np.nan, ' NA':np.nan },\n",
    "                                'subtype':{' NA':np.nan},\n",
    "                                 'age at diagnosis(yrs)':{' NA':np.nan, '>=50':60, '<50':40}})\n",
    "\n",
    "df_total['age at diagnosis(yrs)'] = pd.to_numeric(df_total['age at diagnosis(yrs)'])\n",
    "df_total['tumor size(mm)'] = pd.to_numeric(df_total['tumor size(mm)'])\n",
    "df_total.loc[df_total['tumor size(mm)'] < 20, 'tumor size(mm)'] = 0\n",
    "df_total.loc[df_total['tumor size(mm)'] >= 20, 'tumor size(mm)'] = 1\n",
    "\n",
    "df_total.loc[df_total['age at diagnosis(yrs)'] < 50, 'age at diagnosis(yrs)'] = 0\n",
    "df_total.loc[df_total['age at diagnosis(yrs)'] >= 50, 'age at diagnosis(yrs)'] = 1\n",
    "\n",
    "df_total['stage'] = pd.to_numeric(df_total['stage'])\n",
    "df_total['subtype'] = pd.to_numeric(df_total['subtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing ML model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGM biomarkers\n",
    "with open(f'{MAIN_DIR}/main_data/CGM_biomarkers.pkl', 'rb') as f:\n",
    "    all_genes = pickle.load(f)\n",
    "best_CGM = all_genes[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction dataset (grade 2 and unknown grade samples )\n",
    "df_2_unkown = df_total[~df_total['grade'].isin([1,3])]\n",
    "# developmental dataset (grade 1 and 2)\n",
    "df_3_1 = df_total[df_total['grade'].isin([1,3])]\n",
    "\n",
    "selected_features = best_CGM + ['grade']\n",
    "df_developmental = df_3_1[selected_features]\n",
    "df_prediction = df_2_unkown[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_outlier_detection(X, X_val):\n",
    "    if OUTLIER_DETECTION == 'YES':\n",
    "        import pyod\n",
    "        from pyod.models.knn import KNN\n",
    "        from pyod.utils.data import evaluate_print\n",
    "        from pyod.utils.example import visualize\n",
    "\n",
    "        outlier_fraction = 0.05\n",
    "\n",
    "        clf = KNN(contamination = outlier_fraction)\n",
    "        clf.fit(X)\n",
    "\n",
    "        # get the prediction labels and outlier scores of the training data\n",
    "        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "        y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "        # get the prediction on the val data\n",
    "        y_val_pred = clf.predict(X_val)  # outlier labels (0 or 1)\n",
    "        y_val_scores = clf.decision_function(X_val)  # outlier scores\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        X_val_pca = pca.fit_transform(X_val)\n",
    "\n",
    "        # visualize the results\n",
    "        visualize('KNN', X_pca, y_train_pred, X_val_pca, y_val_pred, y_train_pred,\n",
    "                  y_val_pred, show_figure=True, save_figure=True)\n",
    "        \n",
    "        train_cnt = np.bincount(y_train_pred)\n",
    "        val_cnt = np.bincount(y_val_pred)\n",
    "        print(f'# of inliers in train set: {train_cnt[0]}, # of outliers in train set: {train_cnt[1]}')\n",
    "        print(f'# of inliers in val set: {val_cnt[0]},     # of outliers in val set: {val_cnt[1]}')\n",
    "        \n",
    "        return y_train_pred, y_val_pred, clf\n",
    "    else:\n",
    "        print('Outlier detection is disabled in code options.')\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X, y, NUM_RUNS):\n",
    "    cv_results = cross_validate(model, X, y, cv=NUM_RUNS)\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(X, y, random_state):\n",
    "    print(f\"Before Balancing, counts of label '1': {sum(y == 1)}\")\n",
    "    print(f\"Before Balancing, counts of label '0': {sum(y == 0)}\\n\")\n",
    "\n",
    "    saved_cols = X.columns\n",
    "    if random_state:\n",
    "        sm = SMOTE(random_state=42)\n",
    "    else:\n",
    "        sm = SMOTE()\n",
    "        \n",
    "    X_res, y_res = sm.fit_sample(X.copy(), y.copy().ravel())\n",
    "\n",
    "    print(f\"After Balancing, counts of label '1': {sum(y_res == 1)}\")\n",
    "    print(f\"After Balancing, counts of label '0': {sum(y_res == 0)}\")\n",
    "\n",
    "    return pd.DataFrame(X_res, columns=saved_cols), pd.Series(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(X, y, num_features, fs):\n",
    "     # fs f_classif ---> [f_classif, f_regression, mutual_info_regression,  mutual_info_classif, chi2 ]\n",
    "        \n",
    "    select_k_best_classifier = SelectKBest(score_func=fs, k=num_features)\n",
    "    X_new = select_k_best_classifier.fit_transform(X, y)\n",
    "\n",
    "    mask = select_k_best_classifier.get_support() #list of booleans\n",
    "    new_features = [] # The list of your K best features\n",
    "\n",
    "    for bool, feature in zip(mask, X.columns):\n",
    "        if bool:\n",
    "            new_features.append(feature)\n",
    "\n",
    "    X_new = X[np.intersect1d(X.columns, new_features)]\n",
    "    # Selecting important genes in test set: X_test_new\n",
    "    # X_test_new = X_test[np.intersect1d(X_test.columns, new_features)]\n",
    "    \n",
    "    return num_features, new_features, X_new\n",
    "\n",
    "def select_lasso(X_filtered, y):\n",
    "    # We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "    clf = LassoCV(tol=0.01)\n",
    "\n",
    "    # Set a minimum threshold of 0.25\n",
    "    sfm = SelectFromModel(clf)\n",
    "    sfm.fit(X_filtered, y)\n",
    "    X_new = sfm.transform(X_filtered)\n",
    "    \n",
    "    n_features = X_new.shape[1]\n",
    "    \n",
    "    new_features = []\n",
    "\n",
    "    for i in range(n_features):\n",
    "        for col in X_filtered:\n",
    "            if np.array_equal(X_filtered[col].as_matrix(), X_new[:, i]):\n",
    "                new_features.append(col)     \n",
    "    \n",
    "    print (new_features)\n",
    "    # Selecting important genes in test set: X_test_new\n",
    "    # X_test_new = X_test_Normalized[np.intersect1d(X_test_Normalized.columns, new_features)]\n",
    "    return len(new_features), new_features, X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, y_pred):\n",
    "    result = {}\n",
    "    result['accuracy'] = metrics.accuracy_score(y, y_pred)\n",
    "\n",
    "    try:\n",
    "        result['ROC'] = metrics.roc_auc_score(y, y_pred)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    result['precision'] = precision_score(y, y_pred, average='macro')\n",
    "    result['recall'] = recall_score(y, y_pred, average='macro')\n",
    "    result['f1_score'] = f1_score(y, y_pred, average='macro')                                  \n",
    "    result['matthews_corrcoef'] = matthews_corrcoef(y, y_pred)\n",
    "    try:\n",
    "        TN = confusion_matrix(y, y_pred)[0,0]\n",
    "        FP = confusion_matrix(y, y_pred)[0,1]\n",
    "        specifity = TN/(TN+FP)\n",
    "        dic['specifity'] = specifity\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    result['confusion_matrix'] = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        result['fpr'] = fpr\n",
    "        result['tpr'] = tpr\n",
    "        result['roc_auc'] = roc_auc\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return result\n",
    "\n",
    "def plot_ROC_curve(r):\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(r['fpr'], r['tpr'], color='darkorange', lw=2, label='Neural Network ROC curve (area = %0.2f)' % r['roc_auc'])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    return plt\n",
    "\n",
    "def plot_learning_curve(model):\n",
    "    # retrieve performance metrics\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "    # plot log loss\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.title('XGBoost Log Loss')\n",
    "    pyplot.show()\n",
    "    # plot classification error\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.ylabel('Classification Error')\n",
    "    pyplot.title('XGBoost Classification Error')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(df):\n",
    "    scoring = ['precision_macro', 'recall_macro']\n",
    "    test_results = []\n",
    "    random_state = None\n",
    "    \n",
    "    \n",
    "    df = df.replace({'grade':{1:0}})\n",
    "    df = df.replace({'grade':{3:1}})\n",
    "\n",
    "    best_accuracy, best_model = 0, None\n",
    "    for exp_id in range(0, NUM_RUNS):\n",
    "        print(f'\\n==========================')\n",
    "        print(f'========= Run {exp_id} ==========')\n",
    "        print(f'==========================')\n",
    "\n",
    "        # split into train and test\n",
    "        if random_state:\n",
    "            temp_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "            train_df, val_df = train_test_split(temp_df, test_size=0.1, random_state=42)\n",
    "        else:\n",
    "            temp_df, test_df = train_test_split(df, test_size=0.1)\n",
    "            train_df, val_df = train_test_split(temp_df, test_size=0.1)\n",
    "\n",
    "        train_df_size_1 = train_df[train_df['grade']==0].shape[0] \n",
    "        train_df_size_3 = train_df[train_df['grade']==1].shape[0] \n",
    "        print(f'train_df size: {len(train_df)}, val_df size: {len(val_df)}, test_df size: {len(test_df)}')\n",
    "        print(f'grade-1 : {train_df_size_1}, grade-3: {train_df_size_3}')\n",
    "        y = train_df['grade'].copy()\n",
    "        train_df.drop(columns=['grade'], inplace=True, errors='ignore')\n",
    "\n",
    "        y_val = val_df['grade'].copy()\n",
    "        val_df.drop(columns=['grade'], inplace=True, errors='ignore')\n",
    "\n",
    "        y_test = test_df['grade']\n",
    "        test_df.drop(columns=['grade'], inplace=True, errors='ignore')\n",
    "\n",
    "        # outlier detection\n",
    "        if OUTLIER_DETECTION == 'YES':\n",
    "            print(f'Shape of train_df before OD: {train_df.shape}, val_df: {val_df.shape}')            \n",
    "            y_train_pred, y_val_pred, od_model = perform_outlier_detection(train_df, val_df)\n",
    "\n",
    "            X_mask = y_train_pred == 1\n",
    "            train_df = train_df[~X_mask]\n",
    "            y = y[~X_mask]\n",
    "\n",
    "            X_val_mask = y_val_pred == 1\n",
    "            val_df = val_df[~X_val_mask]\n",
    "            y_val = y_val[~X_val_mask]\n",
    "            print(f'Shape of train_df after OD: {train_df.shape}, val_df: {val_df.shape}')\n",
    "\n",
    "        # balancing\n",
    "        X_balanced, y_balanced = balance_df(train_df, y, random_state)\n",
    "        train_df = X_balanced\n",
    "        new_features = []  \n",
    "\n",
    "        classifier = 'XGBoost'\n",
    "        dic = {'classifier': classifier}\n",
    "\n",
    "        eval_set = [(train_df, y_balanced), (val_df, y_val)]\n",
    "        eval_metric = [\"error\", \"logloss\"]\n",
    "\n",
    "        model = XGBClassifier(colsample_bytree=0.8, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6)\n",
    "        model.fit(train_df, y_balanced, eval_metric=eval_metric, eval_set=eval_set, early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "        plot_learning_curve(model)\n",
    "\n",
    "        y_pred = model.predict(train_df)\n",
    "        train_result = calculate_metrics(y_balanced, y_pred)\n",
    "        print(f'\\ntrain_result: {train_result}')\n",
    "\n",
    "        y_test_pred = model.predict(test_df)        \n",
    "        test_result = calculate_metrics(y_test, y_test_pred)\n",
    "        print(f'\\ntest_result: {test_result}')\n",
    "\n",
    "        test_result['selected_features'] = new_features\n",
    "        test_result['model'] = model\n",
    "\n",
    "        top_features = model.get_booster().get_score(importance_type='weight')\n",
    "        test_result['top_features_weight'] = top_features\n",
    "        top_features = model.get_booster().get_score(importance_type='gain')\n",
    "        test_result['top_features_gain'] = top_features\n",
    "        top_features = model.get_booster().get_score(importance_type='cover')\n",
    "        test_result['top_features_cover'] = top_features\n",
    "\n",
    "        test_results.append(test_result)\n",
    "        \n",
    "    \n",
    "\n",
    "        if test_result['accuracy'] > best_accuracy:\n",
    "            best_model = model\n",
    "        \n",
    "    return test_df, test_results, best_model, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, test_results, best_model, best_accuracy = run_all(df_developmental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test_results = pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_results['f1_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_features_weight = collections.defaultdict(int)\n",
    "for run in df_test_results['top_features_weight']:\n",
    "    for gene, metric in run.items():\n",
    "        all_top_features_weight[gene] += metric\n",
    "sorted_features_lst_weight = sorted(all_top_features_weight.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "df_weight = pd.DataFrame(sorted_features_lst_weight)\n",
    "df_weight.columns = ['gene', 'weight']\n",
    "df_weight.set_index('gene', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_features_gain = collections.defaultdict(int)\n",
    "for run in df_test_results['top_features_gain']:\n",
    "    for gene, metric in run.items():\n",
    "        all_top_features_gain[gene] += metric\n",
    "sorted_features_lst_gain = sorted(all_top_features_gain.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "df_gain = pd.DataFrame(sorted_features_lst_gain)\n",
    "df_gain.columns = ['gene', 'gain']\n",
    "df_gain.set_index('gene', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_top_features_cover = collections.defaultdict(int)\n",
    "for run in df_test_results['top_features_cover']:\n",
    "    for gene, metric in run.items():\n",
    "        all_top_features_cover[gene] += metric      \n",
    "sorted_features_lst_cover = sorted(all_top_features_cover.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "df_cover = pd.DataFrame(sorted_features_lst_gain)\n",
    "df_cover.columns = ['gene', 'cover']\n",
    "df_cover.set_index('gene', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.merge(df_weight, df_gain, left_index=True, right_index=True)\n",
    "df_total = pd.merge(df_total, df_cover, left_index=True, right_index=True)\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shap is for grade2 and unkown howeve we do it here\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(test_df)\n",
    "shap.summary_plot(shap_values, test_df, max_display=74, show=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
